{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DF_DeLt import DeepFace\n",
    "from DF_DeLt.commons import distance as dst\n",
    "from DF_DeLt.detectors import FaceDetector\n",
    "import time\n",
    "import cv2\n",
    "from glob import glob as gl\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from matplotlib import pyplot as plt\n",
    "import threading\n",
    "\n",
    "models = [\"VGG-Face\", \"Facenet\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "model_name = \"Facenet\"\n",
    "recognizor = DeepFace.build_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from yolov3_tf2.models import (\n",
    "    YoloV3, YoloV3Tiny\n",
    ")\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
    "from yolov3_tf2.utils import draw_outputs, convert_output\n",
    "\n",
    "num_classes = 1\n",
    "tfrecord = None\n",
    "weights = 'face-yolov3-tiny.tf'\n",
    "classes = './yolov3_tf2/face.names'\n",
    "size = 416\n",
    "yolo = YoloV3Tiny(classes=num_classes)\n",
    "yolo.load_weights(weights).expect_partial()\n",
    "\n",
    "def face_detection(img_raw):\n",
    "    img_orin = img_raw.copy()\n",
    "    img_raw = tf.convert_to_tensor(img_raw, dtype=tf.uint8)\n",
    "    img = tf.expand_dims(img_raw, 0)\n",
    "    img = transform_images(img, size)\n",
    "    boxes, scores, classes, nums = yolo(img)\n",
    "    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n",
    "    output = convert_output(img, (boxes, scores, classes, nums))\n",
    "    outputs = []\n",
    "    for out in output:\n",
    "        top, right, bottom, left = out\n",
    "        top = 0 if top <0 else top\n",
    "        right = 0 if right <0 else right\n",
    "        bottom = 0 if bottom <0 else bottom\n",
    "        left = 0 if left <0 else left\n",
    "        face = img_orin[top:bottom, left:right]\n",
    "        outputs.append([face, [left, top, right, bottom]])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_names = ['opencv','ssd','mtcnn', 'retinaface']\n",
    "detector_name = 'ssd'\n",
    "detector = FaceDetector.build_model(detector_name)\n",
    "\n",
    "def face_detection(img_raw, align=False, single=True):\n",
    "    faces = FaceDetector.detect_face(detector, detector_name, img_raw, align , single)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(e1, e2):\n",
    "    distance = dst.findEuclideanDistance(dst.l2_normalize(e1), dst.l2_normalize(e2))\n",
    "    return distance\n",
    "\n",
    "def show(img):\n",
    "    plt.imshow(img[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     24,
     33,
     40
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def load_user(path):\n",
    "    images = gl(path+\"/*.jpg\")\n",
    "    info_file = path+\"/info.txt\"\n",
    "    f = open(info_file, \"r\")\n",
    "    content = f.readlines()\n",
    "    info = {\"uid\": content[0][:-1],\n",
    "             \"name\": content[1][:-1],\n",
    "             \"age\": content[2][:-1],\n",
    "             \"permission\": content[3][:-1],\n",
    "             \"more\": content[4][:-1]\n",
    "            }\n",
    "    encoded_faces = []\n",
    "    for image in images:\n",
    "        img = cv2.imread(image)\n",
    "        try:\n",
    "            face = face_detection(img)[0][0]\n",
    "            encoded_face = DeepFace.represent(face, model = recognizor, fast=True)\n",
    "            encoded_faces.append(encoded_face)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return {\"info\":info, \"vectors\": encoded_faces}\n",
    "\n",
    "def load_data():\n",
    "    paths = gl(\"users/*\")\n",
    "    data = {}\n",
    "    for i in tnrange(len(paths)):\n",
    "        path = paths[i]\n",
    "        user = load_user(path)\n",
    "        data[user.get(\"info\").get(\"uid\")] = user\n",
    "    return data\n",
    "\n",
    "def distances_compute(encoded_face, user):\n",
    "    vectors = user['vectors']\n",
    "    distances = []\n",
    "    for vector in vectors:\n",
    "        distances.append(distance(vector, encoded_face))\n",
    "    return np.array(distances)\n",
    "\n",
    "def user_verify(face, threshold = 0.3):\n",
    "    min_distance = 2\n",
    "    found_key = None\n",
    "    user_matched = None\n",
    "    h, w = face.shape[:2]\n",
    "    encoded_face = DeepFace.represent(face, model = recognizor, fast=True)\n",
    "    for key in keys:\n",
    "        distances = distances_compute(encoded_face, data[key])\n",
    "        mean_dis, min_dis = np.mean(distances), np.min(distances)\n",
    "        if min_dis < min_distance:\n",
    "            found_key = key\n",
    "            min_distance = min_dis\n",
    "            user_matched = data[found_key][\"info\"]\n",
    "    if min_distance<threshold:\n",
    "        return min_distance, user_matched\n",
    "    else:\n",
    "        return min_distance, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "test_image = cv2.imread(\"3.jpg\")\n",
    "show(test_image)\n",
    "import time\n",
    "tik = time.time()\n",
    "# faces detection\n",
    "faces = face_detection(test_image)\n",
    "tok = time.time()\n",
    "print(\"detect:\", tok -tik)\n",
    "# face = cv2.imread(\"2.jpg\")\n",
    "for face in faces:\n",
    "    show(face[0])\n",
    "    tik = time.time()\n",
    "    # face verify \n",
    "    min_distance, user_matched = user_verify(face[0], threshold = threshold)\n",
    "    tok = time.time()\n",
    "    print(\"recognition\", tok -tik)\n",
    "    print(min_distance)\n",
    "    print(user_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_trackers = ['BOOSTING', 'MIL', 'KCF','TLD', 'MEDIANFLOW', 'GOTURN', \n",
    "                     'MOSSE', 'CSRT']                                          \n",
    "def generate_tracker(type_of_tracker):\n",
    "    if type_of_tracker == type_of_trackers[0]:\n",
    "        tracker = cv2.legacy.TrackerBoosting_create()\n",
    "    elif type_of_tracker == type_of_trackers[1]:\n",
    "        tracker = cv2.legacy.TrackerMIL_create()\n",
    "    elif type_of_tracker == type_of_trackers[2]:\n",
    "        tracker = cv2.legacy.TrackerKCF_create()\n",
    "    elif type_of_tracker == type_of_trackers[3]:\n",
    "        tracker = cv2.legacy.TrackerTLD_create()\n",
    "    elif type_of_tracker == type_of_trackers[4]:\n",
    "        tracker = cv2.legacy.TrackerMedianFlow_create()\n",
    "    elif type_of_tracker == type_of_trackers[5]:\n",
    "        tracker = cv2.legacy.TrackerGOTURN_create()\n",
    "    elif type_of_tracker == type_of_trackers[6]:\n",
    "        tracker = cv2.legacy.TrackerMOSSE_create()\n",
    "    elif type_of_tracker == type_of_trackers[7]:\n",
    "        tracker = cv2.legacy.TrackerCSRT_create()\n",
    "    else:\n",
    "        tracker = None\n",
    "        print('The name of the tracker is incorrect')\n",
    "        print('Here are the possible trackers:')\n",
    "        for track_type in type_of_trackers:\n",
    "              print(track_type)\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def deep_learning():\n",
    "    print(\"Deep Face threading is started!\")\n",
    "    models = [\"VGG-Face\", \"Facenet\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "    model_name = \"Facenet\"\n",
    "    recognizor = DeepFace.build_model(model_name)\n",
    "    threshold = 0.85\n",
    "    while cf.RUN:\n",
    "        keys = cf.online_streams\n",
    "        if len(keys)>0:\n",
    "            for key in keys:\n",
    "                outputs = []\n",
    "                faces = face_detection(cf.inputs[key])\n",
    "                for face in faces: \n",
    "                    face, pos = face[0], face[1]\n",
    "                    if face.shape[0]*face.shape[1]>0:\n",
    "                        min_distance, user_matched = user_verify(face, threshold = threshold)\n",
    "                        t6 = time.time()\n",
    "                        user, rect_color = [user_matched['name'], (0, 255, 0)] if user_matched is not None else [\"Unknown\", (255, 0, 0)]\n",
    "                        if cf.use_tracking:\n",
    "                            box = [pos[0], pos[1], pos[2]-pos[0], pos[3]-pos[1]]\n",
    "                            tracker = tracker = generate_tracker(cf.desired_tracker)\n",
    "                            tracker.init(cf.inputs[key], box)\n",
    "                            outputs.append([[pos, user, min_distance, rect_color], tracker])\n",
    "                        else:\n",
    "                            outputs.append([[pos, user, min_distance, rect_color], None])\n",
    "                inference_time = time.time()\n",
    "                cf.outputs[key] = [outputs, inference_time]\n",
    "        else:\n",
    "            time.sleep(0.01)\n",
    "    print(\"deep_learning threading is stopped!\")\n",
    "    \n",
    "def video_stream_process(thread_name, link_index):\n",
    "    print(\"Opening video stream \", thread_name)\n",
    "    cap = cv2.VideoCapture(cf.video_links[link_index])\n",
    "    while cf.RUN:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cf.inputs[cf.video_links[link_index]] = frame\n",
    "        if not cf.video_links[link_index] in  cf.online_streams:\n",
    "            cf.online_streams.append(cf.video_links[link_index])\n",
    "        time.sleep(0.025) \n",
    "    print(\"Video stream \", thread_name, \"is stopped!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cf\n",
    "\n",
    "import threading\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "link_index = 0\n",
    "cf.desired_tracker = type_of_trackers[2]\n",
    "cf.RUN = True\n",
    "cf.video_links = [\"1.mp4\", \"2.mp4\", \"3.mp4\"]\n",
    "cf.inputs = {}\n",
    "cf.outputs = {}\n",
    "cf.online_streams = []\n",
    "cf.use_tracking = True\n",
    "\n",
    "deep_learning_thread = threading.Thread(target=deep_learning)\n",
    "deep_learning_thread.start()\n",
    "time.sleep(0.2)\n",
    "video_stream_process_thread_1 = threading.Thread(target=video_stream_process, args=(\"link 1\", 0))\n",
    "video_stream_process_thread_1.start()\n",
    "time.sleep(0.2)\n",
    "video_stream_process_thread_2 = threading.Thread(target=video_stream_process, args=(\"link 2\", 1))\n",
    "video_stream_process_thread_2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# fontScale\n",
    "fontScale = 0.7\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "link = cf.video_links[link_index]\n",
    "while True:\n",
    "    if link in cf.online_streams:\n",
    "        img = cf.inputs[link]\n",
    "        for output in cf.outputs[link][0]:\n",
    "            pos, user, min_distance, rect_color = output[0]\n",
    "            tracker = output[1]\n",
    "            if cf.use_tracking:\n",
    "                s, box = tracker.update(img)\n",
    "            pos = [int(box[0]), int(box[1]), int(box[0])+int(box[2]), int(box[1])+int(box[3])]\n",
    "            cv2.rectangle(img, (pos[0], pos[1]), (pos[2], pos[3]), rect_color, 2)\n",
    "            cv2.putText(img, user +\"(\"+str(round(min_distance, 2))+\")\", (pos[0], pos[1]-10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Online Streams: \"+str(cf.online_streams), (30, 30), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(img, \"Current Stream: \"+link, (30, 60), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.imshow(\"frame\", img)\n",
    "        k = cv2.waitKey(33)\n",
    "        if k == ord('q'):\n",
    "            cf.RUN =False\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        if k == 32:\n",
    "            link_index+=1\n",
    "            link_index = link_index%len(cf.online_streams)\n",
    "            link = cf.online_streams[link_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Camera",
   "language": "python",
   "name": "camera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
