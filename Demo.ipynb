{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import face_recognition\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from glob import glob as gl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepface import DeepFace\n",
    "from deepface.commons import distance as dst\n",
    "import time\n",
    "import cv2\n",
    "from glob import glob as gl\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from yolov3_tf2.models import (\n",
    "    YoloV3, YoloV3Tiny\n",
    ")\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
    "from yolov3_tf2.utils import draw_outputs, convert_output\n",
    "\n",
    "num_classes = 1\n",
    "tfrecord = None\n",
    "weights = 'face-yolov3-tiny.tf'\n",
    "classes = './yolov3_tf2/face.names'\n",
    "size = 416\n",
    "yolo = YoloV3Tiny(classes=num_classes)\n",
    "yolo.load_weights(weights).expect_partial()\n",
    "\n",
    "models = [\"VGG-Face\", \"Facenet\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\"]\n",
    "model_name = \"Dlib\"\n",
    "model = DeepFace.build_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(img_raw):\n",
    "    img_raw = tf.convert_to_tensor(img_raw, dtype=tf.uint8)\n",
    "    img = tf.expand_dims(img_raw, 0)\n",
    "    img = transform_images(img, size)\n",
    "    boxes, scores, classes, nums = yolo(img)\n",
    "    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n",
    "    output = convert_output(img, (boxes, scores, classes, nums))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(e1, e2):\n",
    "    distance = dst.findEuclideanDistance(dst.l2_normalize(e1), dst.l2_normalize(e2))\n",
    "    return distance\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_user(path):\n",
    "    images = gl(path+\"/*.jpg\")\n",
    "    info_file = path+\"/info.txt\"\n",
    "    f = open(info_file, \"r\")\n",
    "    content = f.readlines()\n",
    "    info = {\"uid\": content[0][:-1],\n",
    "             \"name\": content[1][:-1],\n",
    "             \"age\": content[2][:-1],\n",
    "             \"permission\": content[3][:-1],\n",
    "             \"more\": content[4][:-1]\n",
    "            }\n",
    "    encoded_faces = []\n",
    "    for image in images:\n",
    "        img = cv2.imread(image)\n",
    "        encoded_face = DeepFace.represent(img, model = model, fast=True)\n",
    "        encoded_faces.append(encoded_face)\n",
    "        \n",
    "    return {\"info\":info, \"vectors\": encoded_faces}\n",
    "        \n",
    "def load_data():\n",
    "    paths = gl(\"users/*\")\n",
    "    data = {}\n",
    "    for path in paths:\n",
    "        user = load_user(path)\n",
    "        data[user.get(\"info\").get(\"uid\")] = user\n",
    "    return data\n",
    "\n",
    "def distances_compute(face, user):\n",
    "    h, w = face.shape[:2]\n",
    "    encoded_face = DeepFace.represent(face, model = model, fast=True)\n",
    "    vectors = user['vectors']\n",
    "    distances = []\n",
    "    for vector in vectors:\n",
    "        distances.append(distance(vector, encoded_face))\n",
    "    \n",
    "    return np.array(distances)\n",
    "\n",
    "def user_verify(face, threshold = 0.4):\n",
    "    min_distance = 2\n",
    "    found_key = None\n",
    "    user_matched = None\n",
    "    for key in keys:\n",
    "        distances = distances_compute(face, data[key])\n",
    "        mean_dis, min_dis = np.mean(distances), np.min(distances)\n",
    "        if min_dis < min_distance:\n",
    "            found_key = key\n",
    "            min_distance = min_dis\n",
    "            user_matched = data[found_key][\"info\"]\n",
    "    if min_distance<threshold:\n",
    "        return min_distance, user_matched\n",
    "    else:\n",
    "        return min_distance, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = face_recognition.load_image_file(\"1.jpg\")\n",
    "import time\n",
    "tik = time.time()\n",
    "# faces detection\n",
    "face_location = face_detection(test_image)[0]\n",
    "tok = time.time()\n",
    "print(tok -tik)\n",
    "top, right, bottom, left = face_location\n",
    "# crop face\n",
    "face = test_image[top:bottom, left:right]\n",
    "# face = cv2.imread(\"2.jpg\")\n",
    "plt.imshow(face)\n",
    "plt.show()\n",
    "tik = time.time()\n",
    "# face verify \n",
    "min_distance, user_matched = user_verify(face, threshold = 1)\n",
    "tok = time.time()\n",
    "print(tok -tik)\n",
    "print(min_distance)\n",
    "print(user_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# fontScale\n",
    "fontScale = 0.7\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "video_link = \"1.mp4\"\n",
    "cap = cv2.VideoCapture(video_link)\n",
    "stt = 0\n",
    "t1 = time.time()\n",
    "while True:\n",
    "    t2 = time.time()\n",
    "    fps = 1/(t2-t1)\n",
    "    t1 = t2\n",
    "    stt+=1\n",
    "    ret, frame = cap.read()\n",
    "#     frame = cv2.resize(frame, (700, 400))\n",
    "    if not ret:\n",
    "        break\n",
    "    t3 = time.time()\n",
    "    face_locations = face_detection(frame)\n",
    "    t4 = time.time()\n",
    "    N = len(face_locations)\n",
    "    if N>0:\n",
    "        for face_location in face_locations:\n",
    "            top, right, bottom, left = face_location\n",
    "            face = frame[top:bottom, left:right][:, :, ::-1]\n",
    "            t5 = time.time()\n",
    "            min_distance, user_matched = user_verify(face)\n",
    "            t6 = time.time()\n",
    "            user, rect_color = [user_matched['name'], (0, 255, 0)] if user_matched is not None else [\"Unknown\", (255, 0, 0)]\n",
    "            if min_distance>0.6:\n",
    "                rect_color = (0, 0, 255)\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), rect_color, 2)\n",
    "            cv2.putText(frame, user +\"(\"+str(round(min_distance, 2))+\")\", (left, top -10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS: \"+str(round(fps)), (30, 30), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS2: \"+str(round(t4-t3, 4)), (30, 60), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS3: \"+str(round(t6-t5, 4)), (30, 90), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Camera",
   "language": "python",
   "name": "camera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
