{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import face_recognition\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from glob import glob as gl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "model_path = \"export_models/RFB/\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     20,
     28,
     37,
     52
    ]
   },
   "outputs": [],
   "source": [
    "def load_user(path):\n",
    "    images = gl(path+\"/*.jpg\")\n",
    "    info_file = path+\"/info.txt\"\n",
    "    f = open(info_file, \"r\")\n",
    "    content = f.readlines()\n",
    "    info = {\"uid\": content[0][:-1],\n",
    "             \"name\": content[1][:-1],\n",
    "             \"age\": content[2][:-1],\n",
    "             \"permission\": content[3][:-1],\n",
    "             \"more\": content[4][:-1]\n",
    "            }\n",
    "    encoded_faces = []\n",
    "    for image in images:\n",
    "        img = face_recognition.load_image_file(image)\n",
    "        h, w = img.shape[:2]\n",
    "        encoded_face = face_recognition.face_encodings(img, known_face_locations=[(0, w, h, 0)])[0]\n",
    "        encoded_faces.append(encoded_face)\n",
    "        \n",
    "    return {\"info\":info, \"vectors\": encoded_faces}\n",
    "        \n",
    "def load_data():\n",
    "    paths = gl(\"users/*\")\n",
    "    data = {}\n",
    "    for path in paths:\n",
    "        user = load_user(path)\n",
    "        data[user.get(\"info\").get(\"uid\")] = user\n",
    "    return data\n",
    "\n",
    "def distances_compute(face, user):\n",
    "    h, w = face.shape[:2]\n",
    "    encoded_face = face_recognition.face_encodings(face, known_face_locations=[(0, w, h, 0)])[0]\n",
    "    vectors = user['vectors']\n",
    "    \n",
    "    distances = face_recognition.face_distance(vectors, encoded_face)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def user_verify(face):\n",
    "    min_distance = 2\n",
    "    found_key = None\n",
    "    user_matched = None\n",
    "    for key in keys:\n",
    "        distances = distances_compute(face, data[key])\n",
    "        mean_dis, min_dis = np.mean(distances), np.min(distances)\n",
    "        if min_dis < min_distance:\n",
    "            found_key = key\n",
    "            min_distance = min_dis\n",
    "            user_matched = data[found_key][\"info\"]\n",
    "    if min_distance<0.4:\n",
    "        return min_distance, user_matched\n",
    "    else:\n",
    "        return min_distance, None\n",
    "def face_detection(img):\n",
    "    h, w, _ = img.shape\n",
    "    img_resize = cv2.resize(img, (320, 240))\n",
    "    img_resize = cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB)\n",
    "    img_resize = img_resize - 127.0\n",
    "    img_resize = img_resize / 128.0\n",
    "    results = model.predict(np.expand_dims(img_resize, axis=0))  # result=[background,face,x1,y1,x2,y2]\n",
    "    output = []\n",
    "    for result in results:\n",
    "        left = int(result[2] * w)\n",
    "        top = int(result[3] * h)\n",
    "        right = int(result[4] * w)\n",
    "        bottom = int(result[5] * h)\n",
    "        output.append([top, right, bottom, left])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_recognition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-15f2d01aa0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8a8a9c1a211b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8a8a9c1a211b>\u001b[0m in \u001b[0;36mload_user\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mencoded_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mencoded_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'face_recognition' is not defined"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = face_recognition.load_image_file(\"1.jpg\")\n",
    "\n",
    "import time\n",
    "# tik = time.time()\n",
    "face_location = face_detection(test_image)[0]\n",
    "top, right, bottom, left = face_location\n",
    "face = test_image[top:bottom, left:right]\n",
    "tik = time.time()\n",
    "min_distance, user_matched = user_verify(face)\n",
    "tok = time.time()\n",
    "print(tok -tik)\n",
    "print(user_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# fontScale\n",
    "fontScale = 0.7\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "video_link = \"1.mp4\"\n",
    "cap = cv2.VideoCapture(video_link)\n",
    "stt = 0\n",
    "t1 = time.time()\n",
    "while True:\n",
    "    t2 = time.time()\n",
    "    fps = 1/(t2-t1)\n",
    "    t1 = t2\n",
    "    stt+=1\n",
    "    ret, frame = cap.read()\n",
    "#     frame = cv2.resize(frame, (700, 400))\n",
    "    if not ret:\n",
    "        break\n",
    "    t3 = time.time()\n",
    "    face_locations = face_detection(frame)\n",
    "    t4 = time.time()\n",
    "    N = len(face_locations)\n",
    "    if N>0:\n",
    "        for face_location in face_locations:\n",
    "            top, right, bottom, left = face_location\n",
    "            face = frame[top:bottom, left:right][:, :, ::-1]\n",
    "            t5 = time.time()\n",
    "            min_distance, user_matched = user_verify(face)\n",
    "            t6 = time.time()\n",
    "            user, rect_color = [user_matched['name'], (0, 255, 0)] if user_matched is not None else [\"Unknown\", (255, 0, 0)]\n",
    "            if min_distance>0.6:\n",
    "                rect_color = (0, 0, 255)\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), rect_color, 2)\n",
    "            cv2.putText(frame, user +\"(\"+str(round(min_distance, 2))+\")\", (left, top -10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS: \"+str(round(fps)), (30, 30), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS2: \"+str(round(t4-t3, 4)), (30, 60), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"FPS3: \"+str(round(t6-t5, 4)), (30, 90), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Camera_env]",
   "language": "python",
   "name": "conda-env-Camera_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
